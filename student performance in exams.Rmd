---
title: "Students Performance in Exams"
output: html_notebook
---
```{r}
# import necessary libraries
library(data.table)
library(dplyr)
library(ggplot2)
library(gridExtra)

# load the data
df <- fread("/Users/yesollee/Downloads/StudentsPerformance.csv")
```
# Overview of data
```{r}
#overview of data
head(df, 5)
glimpse(df)

#find missing values
colSums(is.na(df))
```
Since there's no missing values, let's dive into data right away.

# Exploratory Data Analysis
## Data preparation
```{r}
# change column names
names(df) <- c("gender", "race", "parental_edu", "lunch", "test_prep", "math", "reading","writing")

# new feature
df[, total := math + reading + writing]

```

## Insights from Data
```{r}
# Descriptive statistics by gender
by(df, df$gender, summary)

# Average score by gender
gender_avg <- df %>%
  group_by(gender) %>%
  summarize(avg_math = mean(math),
            avg_reading = mean(reading),
            avg_writing = mean(writing),
            avg_total = mean(total)) %>%
  ungroup()

print(gender_avg)

a = ggplot(gender_avg, aes(x=gender, y=avg_math, fill=gender)) +
  geom_col() +
  ggtitle("Average score on math by gender")
b = ggplot(gender_avg, aes(x=gender, y=avg_reading, fill=gender)) +
  geom_col() +
  ggtitle("Average score on reading by gender")
c = ggplot(gender_avg, aes(x=gender, y=avg_writing, fill=gender)) +
  geom_col() +
  ggtitle("Average score on writing")
d = ggplot(gender_avg, aes(x=gender, y=avg_total, fill=gender)) +
  geom_col() +
  ggtitle("Average total score on writing")

grid.arrange(a,b,c,d, nrow=2, ncol=2)
```
As you can see from the plot, male students show higher average score on math. But female students show superior score for the rest.

```{r}
# descriptive statistics by race group
by(df, df$race, summary)

# math score boxplot by race group and gender
ggplot(df, aes(x=race, y=math, color=gender)) +
  geom_boxplot()

# reading score boxplot by race group and gender
ggplot(df, aes(x=race, y=reading, color=gender)) +
  geom_boxplot()

# writing score boxplot by race group and gender
ggplot(df, aes(x=race, y=writing, color=gender)) +
  geom_boxplot()

# total boxplot by race group and gender
ggplot(df, aes(x=race, y=total, color=gender)) +
  geom_boxplot()

```

Across all race group, male's median of math score is higher than that of female. Group E shows the highest median in both gender. Regarding reading, group A shows the lowest median in both gender.

```{r}
# parental education level
parent_edu_ct <- df[, .N, by=parental_edu]
setnames(parent_edu_ct, "N", "count")

# parental education level count
ggplot(parent_edu_ct, aes(x = "", y = count, fill = parental_edu)) + 
     geom_bar(width = 1, stat = "identity") + 
     coord_polar(theta = "y", start = 0) + 
     geom_text(aes(label = count), position = position_stack(vjust = 0.5)) + 
     ggtitle("Proportion of parental education level") + 
     theme(plot.title = element_text(hjust = 0.5), 
           axis.ticks = element_blank())

# total score by parent's education level
ggplot(df, aes(x=parental_edu, y=total)) +
  geom_boxplot() +
  ggtitle("Total score distribution by parental education level")

```
Students whose parent have only high school degree showed the lowest median of total score. Students whose parent have master's degree, on the other hand, showed the highest score. The rest of students have pretty similar median score.
```{r}
# lunch and parental_edu
lunch_parentedu <-df %>%
  group_by(lunch, parental_edu) %>%
  summarize(n=n()) %>%
  mutate(freq = n / sum(n))

ggplot(lunch_parentedu, aes(x=parental_edu, y=freq)) +
  geom_col() +
  facet_wrap(~lunch) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

df %>%
  group_by(lunch) %>%
  summarize(avg_math = mean(math),
            avg_reading = mean(reading),
            avg_writing = mean(writing),
            avg_total = mean(total))

```

I expected parents with higher education level have more income therefore use standard lunch rather than free/reduced. To verify this assumption, I grouped by lunch and checked the proportion of each parental education level. The data indicates that there are not much difference in terms of parental education level within the lunch program.
In terms of scores, students who use standard lunch did better on exam across all subjects.

```{r}
# analysis on test preparation program
df[, .N, by=test_prep]

df %>%
  group_by(test_prep) %>%
    summarize(avg_math = mean(math),
            avg_reading = mean(reading),
            avg_writing = mean(writing),
            avg_total = mean(total))
```

Compared to those didn't participate test preparation course, students who completed test_prep showed higher grades in all subjects, especially in writing exam.

# Machine Learning using Tree-Based Models
```{r}
# import necessarry libraries
library(rpart.plot)
library(rpart)
library(Metrics)
library(ipred)
library(caret)
library(randomForest)
library(gbm)
library(fastDummies)

# data splits
set.seed(1)
assignment <- sample(1:3, size = nrow(df), prob = c(7,1.5,1.5), replace = TRUE)

train <- df[assignment == 1, ]
valid <- df[assignment == 2, ]
test <- df[assignment == 3, ] 
```
## Regression Tree
```{r}
# regression tree
# initiate the model
rtree <- rpart(formula = total ~ gender + race + parental_edu + lunch + test_prep, 
                     data = train, 
                     method = "anova")

print(rtree)

rpart.plot(x = rtree, yesno = 2, type = 0, extra = 0)


# hyperparameter tuning
# complexity-cost
plotcp(rtree)
print(rtree$cptable)

opt_index <- which.min(rtree$cptable[, "xerror"])
cp_opt <- rtree$cptable[opt_index, "CP"]

opt_rtree <- prune(tree = rtree, cp = cp_opt)

rpart.plot(x = opt_rtree, yesno = 2, type = 0, extra = 0)

#grid search
minsplit <- seq(1, 4, 1)
maxdepth <- seq(1, 6, 1)

hyper_grid_rt <- expand.grid(minsplit = minsplit, maxdepth = maxdepth)

num_models <- nrow(hyper_grid)
rtree_models <- list()

for (i in 1:num_models) {

    minsplit <- hyper_grid_rt$minsplit[i]
    maxdepth <- hyper_grid_rt$maxdepth[i]

    rtree_models[[i]] <- rpart(formula = total ~ gender + race + parental_edu + lunch + test_prep,
                               data = train, 
                               method = "anova",
                               minsplit = minsplit,
                               maxdepth = maxdepth)
}

#evaluate the models
num_models <- length(rtree_models)
rmse_values <- c()

for (i in 1:num_models) {

    model <- rtree_models[[i]]
    
    pred <- predict(object = model,
                    newdata = valid)
    
    rmse_values[i] <- rmse(actual = valid$total, 
                           predicted = pred)
}

best_rtree <- rtree_models[[which.min(rmse_values)]]

# Print the model paramters of the best model
best_rtree$control

# Compute test set RMSE on best_model
rtree_best_pred <- predict(object = best_rtree,
                newdata = test)
rmse(actual = test$total, 
     predicted = rtree_best_pred)

```
## Bagged Trees
```{r}
# initiate model
set.seed(1)
bagged <- bagging(formula = total ~ gender + race + parental_edu + lunch + test_prep,
                        data = train,
                        coob = TRUE)

print(bagged)

# cross validation for bagged trees using caret
ctrl <- trainControl(method = "cv", number = 5) 

bagged_caret <- train(total ~ gender + race + parental_edu + lunch + test_prep,
                            data = train, 
                            method = "treebag",
                            metric = "RMSE",
                            trControl = ctrl)

print(bagged_caret)

bagged_caret_pred_test <- predict(object = bagged_caret, 
                newdata = test)

rmse(actual = test$total, 
     predicted = bagged_caret_pred_test)
```
## Random forest
```{r}
# initiate the model
rf <- randomForest(formula = total ~ gender + race + parental_edu + lunch + test_prep, data = train)
print(rf)

# predict and compute RMSE
rf_pred <- predict(object = rf, newdata = test)

rmse(actual = test$total,
     predicted = rf_pred)
```
## Boosted Trees
```{r}
train_dummies <- dummy_cols(train, remove_selected_columns = TRUE)
colnames(train_dummies)[colnames(train_dummies) == 'race_group A'] <- 'race_A'
colnames(train_dummies)[colnames(train_dummies) == 'race_group B'] <- 'race_B'
colnames(train_dummies)[colnames(train_dummies) == 'race_group C'] <- 'race_C'
colnames(train_dummies)[colnames(train_dummies) == 'race_group D'] <- 'race_D'
colnames(train_dummies)[colnames(train_dummies) == 'race_group E'] <- 'race_E'
colnames(train_dummies)[colnames(train_dummies) == "parental_edu_associate's degree"] <- 'pe_a'
colnames(train_dummies)[colnames(train_dummies) == "parental_edu_bachelor's degree"] <- 'pe_b'
colnames(train_dummies)[colnames(train_dummies) == "parental_edu_high school"] <- 'pe_h'
colnames(train_dummies)[colnames(train_dummies) == "parental_edu_master's degree"] <- 'pe_m'
colnames(train_dummies)[colnames(train_dummies) == "parental_edu_some college"] <- 'pe_c'
colnames(train_dummies)[colnames(train_dummies) == "parental_edu_some high school"] <- 'pe_sh'
colnames(train_dummies)[colnames(train_dummies) == "lunch_free/reduced"] <- 'lunch_fr'

test_dummies <- dummy_cols(test, remove_selected_columns = TRUE)
colnames(test_dummies)[colnames(test_dummies) == 'race_group A'] <- 'race_A'
colnames(test_dummies)[colnames(test_dummies) == 'race_group B'] <- 'race_B'
colnames(test_dummies)[colnames(test_dummies) == 'race_group C'] <- 'race_C'
colnames(test_dummies)[colnames(test_dummies) == 'race_group D'] <- 'race_D'
colnames(test_dummies)[colnames(test_dummies) == 'race_group E'] <- 'race_E'
colnames(test_dummies)[colnames(test_dummies) == "parental_edu_associate's degree"] <- 'pe_a'
colnames(test_dummies)[colnames(test_dummies) == "parental_edu_bachelor's degree"] <- 'pe_b'
colnames(test_dummies)[colnames(test_dummies) == "parental_edu_high school"] <- 'pe_h'
colnames(test_dummies)[colnames(test_dummies) == "parental_edu_master's degree"] <- 'pe_m'
colnames(test_dummies)[colnames(test_dummies) == "parental_edu_some college"] <- 'pe_c'
colnames(test_dummies)[colnames(test_dummies) == "parental_edu_some high school"] <- 'pe_sh'
colnames(test_dummies)[colnames(test_dummies) == "lunch_free/reduced"] <- 'lunch_fr'

btree <- gbm(formula = total ~ gender_female + gender_male + race_A + race_B + race_C +
              race_D + race_E + pe_a + pe_b + pe_h + pe_m + pe_c + pe_sh + lunch_fr +
              lunch_standard + test_prep_completed + test_prep_none,
              data = train_dummies,
              distribution = "gaussian",
              n.trees = 1000,
              interaction.depth = 2,
              shrinkage = 0.001
             )

print(btree)

# predict
btree_pred <- predict(object=btree, newdata=test_dummies)

rmse(actual = test_dummies$total,
     predicted = btree_pred)

```

# Conclusion
Using various tree-based model, I computed RMSE to evaluate the models. RMSE of each models are followings:

- Regression Tree: 37.15021
- Bagged Tree: 36.65334
- Random Forest: 36.81168
- Boosted Trees: 37.75285

The bagged tree model shows the lowest RMSE, in other words, the model performs best for the current dataset.